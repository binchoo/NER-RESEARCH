{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval==1.0.0 in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.0.0) (0.23.2)\n",
      "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.0.0) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.5.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (3.9.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/bin/sh: 1: Syntax error: \"(\" unexpected\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval==1.0.0\n",
    "!pip install konlpy\n",
    "!bash < (curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PosTag Model\n",
    "- 4그램 단위로 가젯 벡터를 매겨봄. \n",
    "- 임베딩에 7차원 벡터를 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "의존성 주입 헬퍼\n",
    "'''\n",
    "from settings import postag_model_config as config\n",
    "\n",
    "def module_and_class(string):\n",
    "    tok = string.split('.')\n",
    "    mod = \".\".join(tok[0:-1])\n",
    "    cls = tok[-1]\n",
    "    return __import__(mod, fromlist=[cls]), cls\n",
    "\n",
    "def instantiate(string):\n",
    "    mod, cls = module_and_class(string)\n",
    "    return getattr(mod, cls)()\n",
    "\n",
    "mecab = instantiate(config['tagger'])\n",
    "pt = instantiate(config['features'][0])\n",
    "pt.set_tagger(mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4583,
     "status": "ok",
     "timestamp": 1604897378973,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "cdBFg0kb4Urd"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RNN_CRF, self).__init__()\n",
    "\n",
    "        # 전체 음절 개수\n",
    "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
    "\n",
    "        # 음절 임베딩 사이즈\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "\n",
    "        # GRU 히든 사이즈\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        # 분류할 태그의 개수\n",
    "        self.number_of_tags = config[\"number_of_tags\"]\n",
    "        \n",
    "        self.gru_input_size = self.embedding_size + config[\"postag_feature_length\"]\n",
    "\n",
    "        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
    "                                      embedding_dim=self.embedding_size,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # Bi-GRU layer\n",
    "        self.bi_gru = nn.GRU(input_size=self.gru_input_size,\n",
    "                             hidden_size= self.hidden_size,\n",
    "                             num_layers=1,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
    "\n",
    "        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
    "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
    "        self.hidden2num_tag = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_tags)\n",
    "\n",
    "    def forward(self, inputs, postags, labels=None):\n",
    "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
    "        eumjeol_inputs = self.embedding(inputs)\n",
    "        eumjeol_gazette_inputs = torch.cat((eumjeol_inputs, postags), -1)\n",
    "        encoder_outputs, hidden_states = self.bi_gru(eumjeol_gazette_inputs)\n",
    "        \n",
    "        # (batch_size, curr_max_length, hidden_size*2)\n",
    "        d_hidden_outputs = self.dropout(encoder_outputs)\n",
    "\n",
    "        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n",
    "        logits = self.hidden2num_tag(d_hidden_outputs)\n",
    "\n",
    "        if(labels is not None):\n",
    "            log_likelihood = self.crf(emissions=logits,\n",
    "                                      tags=labels,\n",
    "                                      reduction=\"mean\")\n",
    "\n",
    "            loss = log_likelihood * -1.0\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            output = self.crf.decode(emissions=logits)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1604897383072,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "SlVPOWVk4nqP"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def load_vocab(f_name):\n",
    "    vocab_file = open(os.path.join(config['root_dir'], f_name),'r',encoding='utf8')\n",
    "    print(\"{} vocab file loading...\".format(f_name))\n",
    "\n",
    "    # default 요소가 저장된 딕셔너리 생성\n",
    "    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n",
    "\n",
    "    # 시작 인덱스 번호 저장\n",
    "    index = len(symbol2idx)\n",
    "    for line in tqdm(vocab_file.readlines()):\n",
    "        symbol = line.strip()\n",
    "        symbol2idx[symbol] = index\n",
    "        idx2symbol[index]= symbol\n",
    "        index+=1\n",
    "\n",
    "    return symbol2idx, idx2symbol\n",
    "\n",
    "def convert_data2feature(data, symbol2idx, max_length=None):\n",
    "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
    "    words = data.split()\n",
    "\n",
    "    for idx, word in enumerate(words[:max_length]):\n",
    "        if word in symbol2idx.keys():\n",
    "            feature[idx] = symbol2idx[word]\n",
    "        else:\n",
    "            feature[idx] = symbol2idx[\"<UNK>\"]\n",
    "    return feature\n",
    "\n",
    "# 파라미터로 입력받은 파일로부터 tensor객체 생성\n",
    "def load_data(config, f_name, word2idx, tag2idx):\n",
    "    file = open(os.path.join(config['root_dir'], f_name),'r',encoding='utf8')\n",
    "\n",
    "    # return할 문장/라벨 리스트 생성\n",
    "    indexing_inputs, indexing_tags = [], []\n",
    "    postags = []\n",
    "\n",
    "    print(\"{} file loading...\".format(f_name))\n",
    "\n",
    "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
    "    # 문장 \\t 태그\n",
    "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
    "    for line in tqdm(file.readlines()):\n",
    "        try:\n",
    "            id, sentence, tags = line.strip().split('\\t')\n",
    "        except:\n",
    "            id, sentence = line.strip().split('\\t')\n",
    "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
    "        input_postag = pt.transform_end2end(sentence, config[\"max_length\"])\n",
    "        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n",
    "\n",
    "        indexing_inputs.append(input_sentence)\n",
    "        postags.append(input_postag)\n",
    "        indexing_tags.append(indexing_tag)\n",
    "        \n",
    "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
    "    postags = torch.tensor(postags, dtype=torch.float)\n",
    "    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n",
    "\n",
    "    return indexing_inputs, postags, indexing_tags\n",
    "\n",
    "# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n",
    "def tensor2list(input_tensor):\n",
    "    return input_tensor.cpu().detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1604897384165,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "HfKUI3mH4wqh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, TensorDataset)\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(config):\n",
    "    # 모델 객체 생성\n",
    "    model = RNN_CRF(config).cuda()\n",
    "    # 단어 딕셔너리 생성\n",
    "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
    "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
    "\n",
    "    # 데이터 Load\n",
    "    train_input_features, train_postags, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n",
    "    test_input_features, test_postags, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
    "\n",
    "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
    "    train_features = TensorDataset(train_input_features, train_postags, train_tags)\n",
    "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    test_features = TensorDataset(test_input_features, test_postags, test_tags)\n",
    "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # 모델을 학습하기위한 optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "    accuracy_list = []\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # .cuda()를 이용하여 메모리에 업로드\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            input_features, postags, labels = batch\n",
    "\n",
    "            # loss 계산\n",
    "            loss = model.forward(input_features, postags, labels)\n",
    "\n",
    "            # 변화도 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # 모델 내부 각 매개변수 가중치 갱신\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Average Loss : {}\".format(np.mean(losses)))\n",
    "\n",
    "        # 모델 저장\n",
    "        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
    "\n",
    "        do_test(model, test_dataloader, idx2tag)\n",
    "\n",
    "\n",
    "\n",
    "def test(config):\n",
    "    # 모델 객체 생성\n",
    "    model = RNN_CRF(config).cuda()\n",
    "    # 단어 딕셔너리 생성\n",
    "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
    "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
    "\n",
    "\n",
    "    # 저장된 가중치 Load\n",
    "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
    "\n",
    "    # 데이터 Load\n",
    "    test_input_features, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
    "\n",
    "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
    "    test_features = TensorDataset(test_input_features, test_tags)\n",
    "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
    "    # 평가 함수 호출\n",
    "    do_test(model, test_dataloader, idx2tag)\n",
    "\n",
    "def do_test(model, test_dataloader, idx2tag):\n",
    "    model.eval()\n",
    "    predicts, answers = [], []\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        # .cuda() 함수를 이용하요 메모리에 업로드\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        # 데이터를 각 변수에 저장\n",
    "        input_features, postags, labels = batch\n",
    "\n",
    "        # 예측 라벨 출력\n",
    "        output = model(input_features, postags)\n",
    "\n",
    "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
    "        for idx, answer in enumerate(tensor2list(labels)):\n",
    "            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n",
    "            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n",
    "    \n",
    "    # 성능 평가\n",
    "    print(classification_report(answers, predicts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413138,
     "status": "ok",
     "timestamp": 1604897797665,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "QXHsFV-z4zZc",
    "outputId": "8ea46d28-a59d-4e53-e1f7-088a9d24562c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2158/2158 [00:00<00:00, 1549350.91it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 49344.75it/s]\n",
      "  7%|▋         | 479/7319 [00:00<00:01, 4785.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab/word_vocab.txt vocab file loading...\n",
      "vocab/tag_vocab.txt vocab file loading...\n",
      "ner_train.txt file loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7319/7319 [00:01<00:00, 4474.68it/s]\n",
      " 25%|██▌       | 252/995 [00:00<00:00, 2518.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dev.txt file loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [00:00<00:00, 3505.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 step processed.. current loss : 28.995582580566406\n",
      "100 step processed.. current loss : 18.290546417236328\n",
      "Average Loss : 36.88901816658352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.57      0.52      0.55       622\n",
      "          LC       0.66      0.34      0.45       535\n",
      "          OG       0.46      0.21      0.29       971\n",
      "          PS       0.46      0.40      0.43       739\n",
      "          TI       0.29      0.23      0.26        95\n",
      "\n",
      "   micro avg       0.51      0.35      0.41      2962\n",
      "   macro avg       0.49      0.34      0.39      2962\n",
      "weighted avg       0.52      0.35      0.41      2962\n",
      "\n",
      "50 step processed.. current loss : 10.767030715942383\n",
      "100 step processed.. current loss : 9.746017456054688\n",
      "Average Loss : 11.706230868463932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.74      0.69      0.72       622\n",
      "          LC       0.59      0.58      0.58       535\n",
      "          OG       0.66      0.38      0.48       971\n",
      "          PS       0.63      0.55      0.59       739\n",
      "          TI       0.78      0.64      0.71        95\n",
      "\n",
      "   micro avg       0.65      0.53      0.59      2962\n",
      "   macro avg       0.68      0.57      0.61      2962\n",
      "weighted avg       0.66      0.53      0.58      2962\n",
      "\n",
      "50 step processed.. current loss : 7.929391860961914\n",
      "100 step processed.. current loss : 7.151462554931641\n",
      "Average Loss : 8.13886018006698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.71      0.74       622\n",
      "          LC       0.71      0.56      0.62       535\n",
      "          OG       0.68      0.48      0.57       971\n",
      "          PS       0.64      0.60      0.61       739\n",
      "          TI       0.80      0.67      0.73        95\n",
      "\n",
      "   micro avg       0.69      0.58      0.63      2962\n",
      "   macro avg       0.72      0.60      0.65      2962\n",
      "weighted avg       0.70      0.58      0.63      2962\n",
      "\n",
      "50 step processed.. current loss : 6.159181594848633\n",
      "100 step processed.. current loss : 6.263114929199219\n",
      "Average Loss : 6.276866245269775\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.79      0.69      0.74       622\n",
      "          LC       0.76      0.52      0.62       535\n",
      "          OG       0.74      0.43      0.54       971\n",
      "          PS       0.73      0.56      0.63       739\n",
      "          TI       0.83      0.67      0.74        95\n",
      "\n",
      "   micro avg       0.75      0.54      0.63      2962\n",
      "   macro avg       0.77      0.57      0.65      2962\n",
      "weighted avg       0.75      0.54      0.63      2962\n",
      "\n",
      "50 step processed.. current loss : 5.277523040771484\n",
      "100 step processed.. current loss : 6.042940139770508\n",
      "Average Loss : 5.041645796402641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.81      0.73      0.77       622\n",
      "          LC       0.72      0.59      0.65       535\n",
      "          OG       0.69      0.52      0.59       971\n",
      "          PS       0.72      0.63      0.67       739\n",
      "          TI       0.82      0.67      0.74        95\n",
      "\n",
      "   micro avg       0.73      0.61      0.66      2962\n",
      "   macro avg       0.75      0.63      0.68      2962\n",
      "weighted avg       0.73      0.61      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 3.628110885620117\n",
      "100 step processed.. current loss : 2.540729522705078\n",
      "Average Loss : 4.14003323679385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.74      0.75       622\n",
      "          LC       0.54      0.67      0.60       535\n",
      "          OG       0.63      0.56      0.59       971\n",
      "          PS       0.68      0.63      0.65       739\n",
      "          TI       0.76      0.68      0.72        95\n",
      "\n",
      "   micro avg       0.65      0.64      0.64      2962\n",
      "   macro avg       0.68      0.66      0.66      2962\n",
      "weighted avg       0.66      0.64      0.65      2962\n",
      "\n",
      "50 step processed.. current loss : 3.59039306640625\n",
      "100 step processed.. current loss : 3.2320632934570312\n",
      "Average Loss : 3.506103980022928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.79      0.73      0.76       622\n",
      "          LC       0.69      0.62      0.65       535\n",
      "          OG       0.69      0.54      0.60       971\n",
      "          PS       0.71      0.63      0.67       739\n",
      "          TI       0.80      0.72      0.76        95\n",
      "\n",
      "   micro avg       0.72      0.62      0.67      2962\n",
      "   macro avg       0.74      0.65      0.69      2962\n",
      "weighted avg       0.72      0.62      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 4.492429733276367\n",
      "100 step processed.. current loss : 3.6285018920898438\n",
      "Average Loss : 2.996298132772031\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.81      0.73      0.77       622\n",
      "          LC       0.71      0.63      0.67       535\n",
      "          OG       0.67      0.56      0.61       971\n",
      "          PS       0.72      0.63      0.67       739\n",
      "          TI       0.81      0.65      0.72        95\n",
      "\n",
      "   micro avg       0.72      0.63      0.67      2962\n",
      "   macro avg       0.74      0.64      0.69      2962\n",
      "weighted avg       0.72      0.63      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 2.7324047088623047\n",
      "100 step processed.. current loss : 2.027987480163574\n",
      "Average Loss : 2.5396382746489152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.81      0.74      0.78       622\n",
      "          LC       0.72      0.61      0.66       535\n",
      "          OG       0.68      0.56      0.61       971\n",
      "          PS       0.73      0.63      0.68       739\n",
      "          TI       0.77      0.66      0.71        95\n",
      "\n",
      "   micro avg       0.73      0.63      0.67      2962\n",
      "   macro avg       0.74      0.64      0.69      2962\n",
      "weighted avg       0.73      0.63      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 2.2438573837280273\n",
      "100 step processed.. current loss : 2.4460220336914062\n",
      "Average Loss : 2.236455382471499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.76      0.76       622\n",
      "          LC       0.70      0.64      0.67       535\n",
      "          OG       0.67      0.54      0.60       971\n",
      "          PS       0.65      0.68      0.66       739\n",
      "          TI       0.82      0.71      0.76        95\n",
      "\n",
      "   micro avg       0.69      0.64      0.67      2962\n",
      "   macro avg       0.72      0.66      0.69      2962\n",
      "weighted avg       0.70      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.4011354446411133\n",
      "100 step processed.. current loss : 2.218256950378418\n",
      "Average Loss : 1.9617451533027317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.72      0.74       622\n",
      "          LC       0.71      0.64      0.67       535\n",
      "          OG       0.74      0.53      0.62       971\n",
      "          PS       0.68      0.65      0.66       739\n",
      "          TI       0.79      0.69      0.74        95\n",
      "\n",
      "   micro avg       0.72      0.63      0.67      2962\n",
      "   macro avg       0.73      0.65      0.69      2962\n",
      "weighted avg       0.72      0.63      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 2.0959014892578125\n",
      "100 step processed.. current loss : 2.3868513107299805\n",
      "Average Loss : 1.7044355672338733\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.80      0.73      0.76       622\n",
      "          LC       0.72      0.62      0.67       535\n",
      "          OG       0.67      0.59      0.63       971\n",
      "          PS       0.73      0.63      0.68       739\n",
      "          TI       0.77      0.68      0.73        95\n",
      "\n",
      "   micro avg       0.72      0.64      0.68      2962\n",
      "   macro avg       0.74      0.65      0.69      2962\n",
      "weighted avg       0.73      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 1.2459020614624023\n",
      "100 step processed.. current loss : 2.208454132080078\n",
      "Average Loss : 1.5990598118823507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.79      0.73      0.76       622\n",
      "          LC       0.71      0.63      0.67       535\n",
      "          OG       0.68      0.58      0.63       971\n",
      "          PS       0.74      0.65      0.69       739\n",
      "          TI       0.81      0.68      0.74        95\n",
      "\n",
      "   micro avg       0.73      0.64      0.68      2962\n",
      "   macro avg       0.75      0.65      0.70      2962\n",
      "weighted avg       0.73      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9941978454589844\n",
      "100 step processed.. current loss : 1.4640264511108398\n",
      "Average Loss : 1.4463008123895396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.80      0.72      0.75       622\n",
      "          LC       0.73      0.63      0.67       535\n",
      "          OG       0.68      0.58      0.63       971\n",
      "          PS       0.72      0.65      0.69       739\n",
      "          TI       0.78      0.62      0.69        95\n",
      "\n",
      "   micro avg       0.73      0.64      0.68      2962\n",
      "   macro avg       0.74      0.64      0.69      2962\n",
      "weighted avg       0.73      0.64      0.68      2962\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 step processed.. current loss : 1.5862340927124023\n",
      "100 step processed.. current loss : 2.060638427734375\n",
      "Average Loss : 1.3965333948964658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.76      0.73      0.74       622\n",
      "          LC       0.71      0.65      0.68       535\n",
      "          OG       0.65      0.62      0.64       971\n",
      "          PS       0.72      0.65      0.68       739\n",
      "          TI       0.79      0.66      0.72        95\n",
      "\n",
      "   micro avg       0.71      0.66      0.68      2962\n",
      "   macro avg       0.73      0.66      0.69      2962\n",
      "weighted avg       0.71      0.66      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 2.0064945220947266\n",
      "100 step processed.. current loss : 1.7111234664916992\n",
      "Average Loss : 1.2706528363020524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.72      0.73      0.73       622\n",
      "          LC       0.65      0.66      0.65       535\n",
      "          OG       0.63      0.61      0.62       971\n",
      "          PS       0.71      0.65      0.68       739\n",
      "          TI       0.79      0.69      0.74        95\n",
      "\n",
      "   micro avg       0.68      0.66      0.67      2962\n",
      "   macro avg       0.70      0.67      0.68      2962\n",
      "weighted avg       0.68      0.66      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.1886796951293945\n",
      "100 step processed.. current loss : 1.5534167289733887\n",
      "Average Loss : 1.1829357686250106\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.72      0.74       622\n",
      "          LC       0.72      0.62      0.67       535\n",
      "          OG       0.65      0.59      0.62       971\n",
      "          PS       0.72      0.61      0.66       739\n",
      "          TI       0.79      0.63      0.70        95\n",
      "\n",
      "   micro avg       0.71      0.63      0.67      2962\n",
      "   macro avg       0.73      0.64      0.68      2962\n",
      "weighted avg       0.71      0.63      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.8524641990661621\n",
      "100 step processed.. current loss : 1.3283872604370117\n",
      "Average Loss : 1.181769087003625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.73      0.73      0.73       622\n",
      "          LC       0.68      0.65      0.67       535\n",
      "          OG       0.64      0.59      0.61       971\n",
      "          PS       0.71      0.63      0.67       739\n",
      "          TI       0.80      0.66      0.72        95\n",
      "\n",
      "   micro avg       0.69      0.64      0.66      2962\n",
      "   macro avg       0.71      0.65      0.68      2962\n",
      "weighted avg       0.69      0.64      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 0.8704996109008789\n",
      "100 step processed.. current loss : 1.2661428451538086\n",
      "Average Loss : 1.0715115681938503\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.74      0.76       622\n",
      "          LC       0.72      0.64      0.68       535\n",
      "          OG       0.65      0.62      0.64       971\n",
      "          PS       0.71      0.65      0.68       739\n",
      "          TI       0.74      0.71      0.72        95\n",
      "\n",
      "   micro avg       0.71      0.66      0.68      2962\n",
      "   macro avg       0.72      0.67      0.69      2962\n",
      "weighted avg       0.71      0.66      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 1.2872395515441895\n",
      "100 step processed.. current loss : 1.156236171722412\n",
      "Average Loss : 1.0111135721206665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.75      0.76       622\n",
      "          LC       0.73      0.62      0.67       535\n",
      "          OG       0.63      0.63      0.63       971\n",
      "          PS       0.73      0.64      0.68       739\n",
      "          TI       0.76      0.68      0.72        95\n",
      "\n",
      "   micro avg       0.70      0.66      0.68      2962\n",
      "   macro avg       0.72      0.67      0.69      2962\n",
      "weighted avg       0.71      0.66      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9195520877838135\n",
      "100 step processed.. current loss : 1.0720324516296387\n",
      "Average Loss : 0.9426819609559101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.73      0.74       622\n",
      "          LC       0.74      0.63      0.68       535\n",
      "          OG       0.69      0.58      0.63       971\n",
      "          PS       0.70      0.63      0.66       739\n",
      "          TI       0.81      0.71      0.75        95\n",
      "\n",
      "   micro avg       0.72      0.64      0.68      2962\n",
      "   macro avg       0.74      0.66      0.69      2962\n",
      "weighted avg       0.72      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 1.2246472835540771\n",
      "100 step processed.. current loss : 1.1313977241516113\n",
      "Average Loss : 0.9629740911981334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.74      0.75       622\n",
      "          LC       0.66      0.67      0.66       535\n",
      "          OG       0.66      0.60      0.63       971\n",
      "          PS       0.66      0.66      0.66       739\n",
      "          TI       0.78      0.72      0.75        95\n",
      "\n",
      "   micro avg       0.68      0.66      0.67      2962\n",
      "   macro avg       0.70      0.68      0.69      2962\n",
      "weighted avg       0.68      0.66      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.0601446628570557\n",
      "100 step processed.. current loss : 0.7376835346221924\n",
      "Average Loss : 0.9211271493331246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.74      0.72      0.73       622\n",
      "          LC       0.70      0.65      0.68       535\n",
      "          OG       0.67      0.59      0.63       971\n",
      "          PS       0.65      0.65      0.65       739\n",
      "          TI       0.71      0.65      0.68        95\n",
      "\n",
      "   micro avg       0.69      0.65      0.67      2962\n",
      "   macro avg       0.69      0.65      0.67      2962\n",
      "weighted avg       0.69      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9862666130065918\n",
      "100 step processed.. current loss : 1.1726115942001343\n",
      "Average Loss : 0.9593667382779328\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.72      0.75       622\n",
      "          LC       0.75      0.63      0.68       535\n",
      "          OG       0.66      0.61      0.63       971\n",
      "          PS       0.71      0.63      0.67       739\n",
      "          TI       0.74      0.64      0.69        95\n",
      "\n",
      "   micro avg       0.71      0.64      0.68      2962\n",
      "   macro avg       0.73      0.64      0.68      2962\n",
      "weighted avg       0.72      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7710813283920288\n",
      "100 step processed.. current loss : 1.016002893447876\n",
      "Average Loss : 0.9970624042593914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.72      0.75       622\n",
      "          LC       0.71      0.62      0.66       535\n",
      "          OG       0.66      0.60      0.63       971\n",
      "          PS       0.74      0.63      0.68       739\n",
      "          TI       0.70      0.66      0.68        95\n",
      "\n",
      "   micro avg       0.71      0.64      0.67      2962\n",
      "   macro avg       0.72      0.65      0.68      2962\n",
      "weighted avg       0.72      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.0927362442016602\n",
      "100 step processed.. current loss : 0.9200626611709595\n",
      "Average Loss : 1.0139135982679284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.72      0.75       622\n",
      "          LC       0.72      0.62      0.66       535\n",
      "          OG       0.65      0.58      0.62       971\n",
      "          PS       0.71      0.65      0.68       739\n",
      "          TI       0.72      0.64      0.68        95\n",
      "\n",
      "   micro avg       0.71      0.64      0.67      2962\n",
      "   macro avg       0.72      0.64      0.68      2962\n",
      "weighted avg       0.71      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9543148875236511\n",
      "100 step processed.. current loss : 1.154255747795105\n",
      "Average Loss : 0.9776891869047414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <SP> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.74      0.74       622\n",
      "          LC       0.65      0.69      0.67       535\n",
      "          OG       0.66      0.59      0.62       971\n",
      "          PS       0.65      0.66      0.65       739\n",
      "          TI       0.73      0.68      0.71        95\n",
      "\n",
      "   micro avg       0.68      0.66      0.67      2962\n",
      "   macro avg       0.69      0.67      0.68      2962\n",
      "weighted avg       0.68      0.66      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.5981247425079346\n",
      "100 step processed.. current loss : 1.1348077058792114\n",
      "Average Loss : 0.964946718060452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.79      0.74      0.76       622\n",
      "          LC       0.71      0.60      0.65       535\n",
      "          OG       0.69      0.55      0.61       971\n",
      "          PS       0.69      0.62      0.65       739\n",
      "          TI       0.75      0.73      0.74        95\n",
      "\n",
      "   micro avg       0.72      0.62      0.67      2962\n",
      "   macro avg       0.72      0.65      0.68      2962\n",
      "weighted avg       0.72      0.62      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 1.0109138488769531\n",
      "100 step processed.. current loss : 1.6017570495605469\n",
      "Average Loss : 0.8872441776420759\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.72      0.73       622\n",
      "          LC       0.70      0.64      0.67       535\n",
      "          OG       0.64      0.60      0.62       971\n",
      "          PS       0.70      0.63      0.67       739\n",
      "          TI       0.74      0.74      0.74        95\n",
      "\n",
      "   micro avg       0.69      0.64      0.67      2962\n",
      "   macro avg       0.71      0.67      0.69      2962\n",
      "weighted avg       0.69      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.6380207538604736\n",
      "100 step processed.. current loss : 1.1952378749847412\n",
      "Average Loss : 0.8675311619820802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.73      0.75       622\n",
      "          LC       0.71      0.65      0.68       535\n",
      "          OG       0.67      0.57      0.62       971\n",
      "          PS       0.72      0.63      0.67       739\n",
      "          TI       0.80      0.74      0.77        95\n",
      "\n",
      "   micro avg       0.72      0.64      0.68      2962\n",
      "   macro avg       0.74      0.66      0.70      2962\n",
      "weighted avg       0.72      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9260076880455017\n",
      "100 step processed.. current loss : 0.9472923278808594\n",
      "Average Loss : 0.9040062497491422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.73      0.73      0.73       622\n",
      "          LC       0.74      0.63      0.68       535\n",
      "          OG       0.63      0.61      0.62       971\n",
      "          PS       0.71      0.64      0.67       739\n",
      "          TI       0.70      0.67      0.68        95\n",
      "\n",
      "   micro avg       0.69      0.65      0.67      2962\n",
      "   macro avg       0.70      0.66      0.68      2962\n",
      "weighted avg       0.69      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.1498847007751465\n",
      "100 step processed.. current loss : 0.9840254187583923\n",
      "Average Loss : 0.9369699081648951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.76      0.77       622\n",
      "          LC       0.72      0.63      0.67       535\n",
      "          OG       0.71      0.55      0.62       971\n",
      "          PS       0.74      0.64      0.68       739\n",
      "          TI       0.73      0.69      0.71        95\n",
      "\n",
      "   micro avg       0.73      0.64      0.68      2962\n",
      "   macro avg       0.74      0.65      0.69      2962\n",
      "weighted avg       0.73      0.64      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9487307071685791\n",
      "100 step processed.. current loss : 1.107100486755371\n",
      "Average Loss : 0.945019301124241\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.74      0.73      0.74       622\n",
      "          LC       0.65      0.65      0.65       535\n",
      "          OG       0.67      0.60      0.64       971\n",
      "          PS       0.65      0.68      0.67       739\n",
      "          TI       0.76      0.69      0.73        95\n",
      "\n",
      "   micro avg       0.68      0.66      0.67      2962\n",
      "   macro avg       0.69      0.67      0.68      2962\n",
      "weighted avg       0.68      0.66      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7253574132919312\n",
      "100 step processed.. current loss : 0.8511369228363037\n",
      "Average Loss : 0.8864673930665721\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.76      0.73      0.74       622\n",
      "          LC       0.71      0.64      0.67       535\n",
      "          OG       0.67      0.60      0.63       971\n",
      "          PS       0.67      0.67      0.67       739\n",
      "          TI       0.69      0.67      0.68        95\n",
      "\n",
      "   micro avg       0.69      0.65      0.67      2962\n",
      "   macro avg       0.70      0.66      0.68      2962\n",
      "weighted avg       0.69      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7096490859985352\n",
      "100 step processed.. current loss : 0.9825425148010254\n",
      "Average Loss : 0.858023478673852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.79      0.74      0.76       622\n",
      "          LC       0.69      0.63      0.66       535\n",
      "          OG       0.65      0.60      0.62       971\n",
      "          PS       0.69      0.64      0.66       739\n",
      "          TI       0.72      0.68      0.70        95\n",
      "\n",
      "   micro avg       0.70      0.65      0.67      2962\n",
      "   macro avg       0.71      0.66      0.68      2962\n",
      "weighted avg       0.70      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.211899757385254\n",
      "100 step processed.. current loss : 1.0330183506011963\n",
      "Average Loss : 0.8651732133782428\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.78      0.74      0.76       622\n",
      "          LC       0.68      0.63      0.65       535\n",
      "          OG       0.66      0.59      0.62       971\n",
      "          PS       0.70      0.65      0.67       739\n",
      "          TI       0.66      0.66      0.66        95\n",
      "\n",
      "   micro avg       0.70      0.65      0.67      2962\n",
      "   macro avg       0.69      0.65      0.67      2962\n",
      "weighted avg       0.70      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.4787277579307556\n",
      "100 step processed.. current loss : 0.7056005001068115\n",
      "Average Loss : 0.835547383453535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.73      0.75       622\n",
      "          LC       0.70      0.63      0.66       535\n",
      "          OG       0.68      0.57      0.62       971\n",
      "          PS       0.70      0.64      0.67       739\n",
      "          TI       0.70      0.68      0.69        95\n",
      "\n",
      "   micro avg       0.70      0.64      0.67      2962\n",
      "   macro avg       0.71      0.65      0.68      2962\n",
      "weighted avg       0.71      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.1438641548156738\n",
      "100 step processed.. current loss : 0.7711232900619507\n",
      "Average Loss : 0.8788140379864237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.75      0.74      0.74       622\n",
      "          LC       0.72      0.64      0.68       535\n",
      "          OG       0.64      0.58      0.61       971\n",
      "          PS       0.70      0.64      0.67       739\n",
      "          TI       0.70      0.67      0.69        95\n",
      "\n",
      "   micro avg       0.69      0.64      0.67      2962\n",
      "   macro avg       0.70      0.65      0.68      2962\n",
      "weighted avg       0.69      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7631051540374756\n",
      "100 step processed.. current loss : 0.8892438411712646\n",
      "Average Loss : 0.8959434939467389\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.76      0.72      0.74       622\n",
      "          LC       0.72      0.61      0.66       535\n",
      "          OG       0.64      0.60      0.62       971\n",
      "          PS       0.70      0.67      0.68       739\n",
      "          TI       0.73      0.72      0.72        95\n",
      "\n",
      "   micro avg       0.70      0.65      0.67      2962\n",
      "   macro avg       0.71      0.66      0.69      2962\n",
      "weighted avg       0.70      0.65      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 1.1224000453948975\n",
      "100 step processed.. current loss : 0.5493760704994202\n",
      "Average Loss : 0.8544188250666079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.73      0.75       622\n",
      "          LC       0.73      0.64      0.68       535\n",
      "          OG       0.67      0.57      0.61       971\n",
      "          PS       0.73      0.61      0.67       739\n",
      "          TI       0.70      0.68      0.69        95\n",
      "\n",
      "   micro avg       0.72      0.63      0.67      2962\n",
      "   macro avg       0.72      0.65      0.68      2962\n",
      "weighted avg       0.72      0.63      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7375630736351013\n",
      "100 step processed.. current loss : 1.1635046005249023\n",
      "Average Loss : 0.8654801519020744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.77      0.72      0.75       622\n",
      "          LC       0.67      0.64      0.65       535\n",
      "          OG       0.67      0.55      0.60       971\n",
      "          PS       0.68      0.65      0.67       739\n",
      "          TI       0.69      0.66      0.68        95\n",
      "\n",
      "   micro avg       0.69      0.63      0.66      2962\n",
      "   macro avg       0.70      0.64      0.67      2962\n",
      "weighted avg       0.69      0.63      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 1.3212655782699585\n",
      "100 step processed.. current loss : 0.8927018642425537\n",
      "Average Loss : 0.8756225477094236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.74      0.73      0.73       622\n",
      "          LC       0.71      0.62      0.66       535\n",
      "          OG       0.66      0.60      0.63       971\n",
      "          PS       0.71      0.64      0.68       739\n",
      "          TI       0.72      0.69      0.71        95\n",
      "\n",
      "   micro avg       0.70      0.64      0.67      2962\n",
      "   macro avg       0.71      0.66      0.68      2962\n",
      "weighted avg       0.70      0.64      0.67      2962\n",
      "\n",
      "50 step processed.. current loss : 0.4620957374572754\n",
      "100 step processed.. current loss : 1.0630333423614502\n",
      "Average Loss : 0.8713516015073527\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.72      0.70      0.71       622\n",
      "          LC       0.71      0.61      0.66       535\n",
      "          OG       0.64      0.58      0.61       971\n",
      "          PS       0.70      0.65      0.67       739\n",
      "          TI       0.74      0.67      0.70        95\n",
      "\n",
      "   micro avg       0.69      0.63      0.66      2962\n",
      "   macro avg       0.70      0.64      0.67      2962\n",
      "weighted avg       0.69      0.63      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 0.5707387924194336\n",
      "100 step processed.. current loss : 1.6104116439819336\n",
      "Average Loss : 0.8687385693840358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.73      0.71      0.72       622\n",
      "          LC       0.74      0.62      0.68       535\n",
      "          OG       0.68      0.56      0.62       971\n",
      "          PS       0.71      0.62      0.66       739\n",
      "          TI       0.74      0.71      0.72        95\n",
      "\n",
      "   micro avg       0.71      0.62      0.66      2962\n",
      "   macro avg       0.72      0.64      0.68      2962\n",
      "weighted avg       0.71      0.62      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 0.5830802917480469\n",
      "100 step processed.. current loss : 1.4597368240356445\n",
      "Average Loss : 0.9449233480121778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.76      0.75      0.75       622\n",
      "          LC       0.72      0.62      0.66       535\n",
      "          OG       0.69      0.59      0.63       971\n",
      "          PS       0.67      0.67      0.67       739\n",
      "          TI       0.82      0.69      0.75        95\n",
      "\n",
      "   micro avg       0.71      0.65      0.68      2962\n",
      "   macro avg       0.73      0.66      0.69      2962\n",
      "weighted avg       0.71      0.65      0.68      2962\n",
      "\n",
      "50 step processed.. current loss : 0.8053398132324219\n",
      "100 step processed.. current loss : 1.7681694030761719\n",
      "Average Loss : 1.31503267288208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.73      0.74      0.73       622\n",
      "          LC       0.75      0.59      0.66       535\n",
      "          OG       0.64      0.58      0.61       971\n",
      "          PS       0.71      0.62      0.66       739\n",
      "          TI       0.75      0.61      0.67        95\n",
      "\n",
      "   micro avg       0.70      0.62      0.66      2962\n",
      "   macro avg       0.72      0.63      0.67      2962\n",
      "weighted avg       0.70      0.62      0.66      2962\n",
      "\n",
      "50 step processed.. current loss : 3.4302139282226562\n",
      "100 step processed.. current loss : 6.91900634765625\n",
      "Average Loss : 4.010317901943041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.59      0.57      0.58       622\n",
      "          LC       0.69      0.50      0.58       535\n",
      "          OG       0.59      0.40      0.48       971\n",
      "          PS       0.78      0.33      0.46       739\n",
      "          TI       0.61      0.53      0.56        95\n",
      "\n",
      "   micro avg       0.63      0.44      0.52      2962\n",
      "   macro avg       0.65      0.47      0.53      2962\n",
      "weighted avg       0.65      0.44      0.52      2962\n",
      "\n",
      "50 step processed.. current loss : 26.24053955078125\n",
      "100 step processed.. current loss : 9.58856201171875\n",
      "Average Loss : 9.997507053872813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.50      0.54      0.52       622\n",
      "          LC       0.71      0.38      0.49       535\n",
      "          OG       0.63      0.24      0.35       971\n",
      "          PS       0.67      0.36      0.47       739\n",
      "          TI       0.33      0.07      0.12        95\n",
      "\n",
      "   micro avg       0.59      0.35      0.44      2962\n",
      "   macro avg       0.57      0.32      0.39      2962\n",
      "weighted avg       0.62      0.35      0.43      2962\n",
      "\n",
      "50 step processed.. current loss : 8.671173095703125\n",
      "100 step processed.. current loss : 19.19818115234375\n",
      "Average Loss : 13.196193496040676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.01      0.01      0.01       622\n",
      "          LC       0.66      0.19      0.30       535\n",
      "          OG       0.05      0.00      0.01       971\n",
      "          PS       0.70      0.19      0.30       739\n",
      "          TI       0.22      0.06      0.10        95\n",
      "\n",
      "   micro avg       0.18      0.09      0.12      2962\n",
      "   macro avg       0.33      0.09      0.14      2962\n",
      "weighted avg       0.32      0.09      0.14      2962\n",
      "\n",
      "50 step processed.. current loss : 5.890594482421875\n",
      "100 step processed.. current loss : 5.7822265625\n",
      "Average Loss : 10.304978590426238\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.69      0.65      0.67       622\n",
      "          LC       0.74      0.47      0.58       535\n",
      "          OG       0.59      0.43      0.50       971\n",
      "          PS       0.65      0.54      0.59       739\n",
      "          TI       0.70      0.59      0.64        95\n",
      "\n",
      "   micro avg       0.65      0.52      0.58      2962\n",
      "   macro avg       0.67      0.54      0.59      2962\n",
      "weighted avg       0.66      0.52      0.58      2962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                                                        #\n",
    "#        평가 기준이 되는 지표는 Macro F1 Score          #\n",
    "#           제출 포맷은 id \\t predict_tag                #\n",
    "#            25 \\t B_PS I_PS <SP> O O O ...              #\n",
    "#                                                        #\n",
    "##########################################################\n",
    "\n",
    "\n",
    "import os\n",
    "if(__name__==\"__main__\"):\n",
    "    output_dir = os.path.join(config['root_dir'], \"output\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if(config[\"mode\"] == \"train\"):\n",
    "        train(config)\n",
    "    else:\n",
    "        test(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ENjD9mNs5B3U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UNK': 0, 'NNG': 1, 'NNP': 2, 'NNB': 3, 'NNBC': 4, 'NR': 5, 'NP': 6, 'VV': 7, 'VA': 8, 'VX': 9, 'VCP': 10, 'VCN': 11, 'MM': 12, 'MAG': 13, 'MAJ': 14, 'IC': 15, 'JKS': 16, 'JKC': 17, 'JKG': 18, 'JKO': 19, 'JKB': 20, 'JKV': 21, 'JKQ': 22, 'JX': 23, 'JC': 24, 'EP': 25, 'EF': 26, 'EC': 27, 'ETN': 28, 'ETM': 29, 'XPN': 30, 'XSN': 31, 'XSV': 32, 'XSA': 33, 'XR': 34, 'SF': 35, 'SE': 36, 'SSO': 37, 'SSC': 38, 'SC': 39, 'SY': 40, 'SL': 41, 'SH': 42, 'SN': 43}\n"
     ]
    }
   ],
   "source": [
    "print(pt.label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPPhYT+qI42fZ4mJM5g2+tr",
   "collapsed_sections": [],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
