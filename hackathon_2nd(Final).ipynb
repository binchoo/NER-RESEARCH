{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval==1.0.0 in /usr/local/lib/python3.8/dist-packages (1.0.0)\n",
      "Requirement already satisfied: numpy==1.19.2 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.0.0) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.8/dist-packages (from seqeval==1.0.0) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.2->seqeval==1.0.0) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval==1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gazetteer Model\n",
    "- 3~10그램 단위로 가젯 벡터를 매겨봄\n",
    "- 따라서 기존 임베딩에 5 * 8차원 벡터를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mode': 'train', 'root_dir': '.', 'train_file': 'ner_train_dev.txt', 'dev_file': 'ner_dev.txt', 'word_vocab_file': 'vocab/word_vocab.txt', 'tag_vocab_file': 'vocab/tag_vocab.txt', 'trained_model_name': 'epoch_5.pt', 'output_dir_path': './output', 'word_vocab_size': 2160, 'number_of_tags': 14, 'hidden_size': 200, 'dropout': 0.1, 'embedding_size': 200, 'max_length': 150, 'batch_size': 64, 'epoch': 30, 'features': ['feature.gazetteer.GazetteFeature'], 'lr': 0.003, 'gazette_feature_length': 7, 'ngrams': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n"
     ]
    }
   ],
   "source": [
    "from config import GazetteModelConfig\n",
    "\n",
    "config = GazetteModelConfig(epoch=30, lr=0.003, dropout=0.1, train_file='ner_train_dev.txt')\n",
    "gz = config.feature_tool(0)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4583,
     "status": "ok",
     "timestamp": 1604897378973,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "cdBFg0kb4Urd"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "class RNN_CRF(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RNN_CRF, self).__init__()\n",
    "\n",
    "        # 전체 음절 개수\n",
    "        self.eumjeol_vocab_size = config[\"word_vocab_size\"]\n",
    "\n",
    "        # 음절 임베딩 사이즈\n",
    "        self.embedding_size = config[\"embedding_size\"]\n",
    "\n",
    "        # GRU 히든 사이즈\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "\n",
    "        # 분류할 태그의 개수\n",
    "        self.number_of_tags = config[\"number_of_tags\"]\n",
    "        \n",
    "        self.gru_input_size = self.embedding_size + config[\"gazette_feature_length\"] * len(config[\"ngrams\"])\n",
    "\n",
    "        # 입력 데이터에 있는 각 음절 index를 대응하는 임베딩 벡터로 치환해주기 위한 임베딩 객체\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.eumjeol_vocab_size,\n",
    "                                      embedding_dim=self.embedding_size,\n",
    "                                      padding_idx=0)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # Bi-GRU layer\n",
    "        self.bi_gru = nn.GRU(input_size=self.gru_input_size,\n",
    "                             hidden_size= self.hidden_size,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=True)\n",
    "        \n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags=self.number_of_tags, batch_first=True)\n",
    "\n",
    "        # fully_connected layer를 통하여 출력 크기를 number_of_tags에 맞춰줌\n",
    "        # (batch_size, max_length, hidden_size*2) -> (batch_size, max_length, number_of_tags)\n",
    "        self.hidden2num_tag = nn.Linear(in_features=self.hidden_size*2, out_features=self.number_of_tags)\n",
    "\n",
    "    def forward(self, inputs, gazettes, labels=None):\n",
    "        # (batch_size, max_length) -> (batch_size, max_length, embedding_size)\n",
    "        eumjeol_inputs = self.embedding(inputs)\n",
    "        eumjeol_gazette_inputs = torch.cat((eumjeol_inputs, gazettes), -1)\n",
    "        \n",
    "        encoder_outputs, hidden_states = self.bi_gru(eumjeol_gazette_inputs)\n",
    "        \n",
    "        # (batch_size, curr_max_length, hidden_size*2)\n",
    "        d_hidden_outputs = self.dropout(encoder_outputs)\n",
    "\n",
    "        # (batch_size, curr_max_length, hidden_size*2) -> (batch_size, curr_max_length, number_of_tags)\n",
    "        logits = self.hidden2num_tag(d_hidden_outputs)\n",
    "\n",
    "        if(labels is not None):\n",
    "            log_likelihood = self.crf(emissions=logits,\n",
    "                                      tags=labels,\n",
    "                                      reduction=\"mean\")\n",
    "\n",
    "            loss = log_likelihood * -1.0\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            output = self.crf.decode(emissions=logits)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1604897383072,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "SlVPOWVk4nqP"
   },
   "outputs": [],
   "source": [
    "from preprocess.replacement import replace_digit\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def load_vocab(f_name):\n",
    "    vocab_file = open(os.path.join(config['root_dir'], f_name),'r',encoding='utf8')\n",
    "    print(\"{} vocab file loading...\".format(f_name))\n",
    "\n",
    "    # default 요소가 저장된 딕셔너리 생성\n",
    "    symbol2idx, idx2symbol = {\"<PAD>\":0, \"<UNK>\":1}, {0:\"<PAD>\", 1:\"<UNK>\"}\n",
    "\n",
    "    # 시작 인덱스 번호 저장\n",
    "    index = len(symbol2idx)\n",
    "    for line in tqdm(vocab_file.readlines()):\n",
    "        symbol = line.strip()\n",
    "        symbol2idx[symbol] = index\n",
    "        idx2symbol[index]= symbol\n",
    "        index+=1\n",
    "\n",
    "    return symbol2idx, idx2symbol\n",
    "\n",
    "def convert_data2feature(data, symbol2idx, max_length=None):\n",
    "    feature = np.zeros(shape=(max_length), dtype=np.int)\n",
    "    words = data.split()\n",
    "\n",
    "    for idx, word in enumerate(words[:max_length]):\n",
    "        if word in symbol2idx.keys():\n",
    "            feature[idx] = symbol2idx[word]\n",
    "        else:\n",
    "            feature[idx] = symbol2idx[\"<UNK>\"]\n",
    "    return feature\n",
    "\n",
    "# 파라미터로 입력받은 파일로부터 tensor객체 생성\n",
    "def load_data(config, f_name, word2idx, tag2idx):\n",
    "    global original_lines\n",
    "    file = open(os.path.join(config['root_dir'], f_name),'r',encoding='utf8')\n",
    "\n",
    "    # return할 문장/라벨 리스트 생성\n",
    "    indexing_inputs, indexing_tags = [], []\n",
    "    gazettes = []\n",
    "\n",
    "    print(\"{} file loading...\".format(f_name))\n",
    "\n",
    "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
    "    # 문장 \\t 태그\n",
    "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
    "    original_lines = file.readlines()\n",
    "    for line in tqdm(original_lines):\n",
    "        line = replace_digit(line)\n",
    "        try:\n",
    "            id, sentence, tags = line.strip().split('\\t')\n",
    "        except:\n",
    "            id, sentence = line.strip().split('\\t')\n",
    "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
    "        input_gazette = gz.transform_end2end_complex_ngrams(sentence, config[\"max_length\"], config[\"ngrams\"])\n",
    "        indexing_tag = convert_data2feature(tags, tag2idx, config[\"max_length\"])\n",
    "\n",
    "        indexing_inputs.append(input_sentence)\n",
    "        gazettes.append(input_gazette)\n",
    "        indexing_tags.append(indexing_tag)\n",
    "        \n",
    "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
    "    gazettes = torch.tensor(gazettes, dtype=torch.float)\n",
    "    indexing_tags = torch.tensor(indexing_tags, dtype=torch.long)\n",
    "\n",
    "    return indexing_inputs, gazettes, indexing_tags\n",
    "\n",
    "# tensor 객체를 리스트 형으로 바꾸기 위한 함수\n",
    "def tensor2list(input_tensor):\n",
    "    return input_tensor.cpu().detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1604897384165,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "HfKUI3mH4wqh"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, TensorDataset, RandomSampler)\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(config):\n",
    "    # 모델 객체 생성\n",
    "    model = RNN_CRF(config).cuda()\n",
    "    # 단어 딕셔너리 생성\n",
    "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
    "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
    "\n",
    "    # 데이터 Load\n",
    "    train_input_features, train_gazettes, train_tags = load_data(config, config[\"train_file\"], word2idx, tag2idx)\n",
    "    test_input_features, test_gazettes, test_tags = load_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
    "\n",
    "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
    "    train_features = TensorDataset(train_input_features, train_gazettes, train_tags)\n",
    "    train_dataloader = DataLoader(train_features, shuffle=True, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    test_features = TensorDataset(test_input_features, test_gazettes, test_tags)\n",
    "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # 모델을 학습하기위한 optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    accuracy_list = []\n",
    "    for epoch in range(config[\"epoch\"]):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # .cuda()를 이용하여 메모리에 업로드\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            input_features, gazettes, labels = batch\n",
    "\n",
    "            # loss 계산\n",
    "            loss = model.forward(input_features, gazettes, labels)\n",
    "\n",
    "            # 변화도 초기화\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # loss 값으로부터 모델 내부 각 매개변수에 대하여 gradient 계산\n",
    "            loss.backward()\n",
    "\n",
    "            # 모델 내부 각 매개변수 가중치 갱신\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % 50 == 0:\n",
    "                print(\"{} step processed.. current loss : {}\".format(step + 1, loss.data.item()))\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Average Loss : {}\".format(np.mean(losses)))\n",
    "\n",
    "        # 모델 저장\n",
    "        torch.save(model.state_dict(), os.path.join(config[\"output_dir_path\"], \"epoch_{}.pt\".format(epoch + 1)))\n",
    "\n",
    "        do_test(model, test_dataloader, idx2tag)\n",
    "\n",
    "def do_test(model, test_dataloader, idx2tag):\n",
    "    model.eval()\n",
    "    predicts, answers = [], []\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        # .cuda() 함수를 이용하요 메모리에 업로드\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "\n",
    "        # 데이터를 각 변수에 저장\n",
    "        input_features, gazettes, labels = batch\n",
    "\n",
    "        # 예측 라벨 출력\n",
    "        output = model(input_features, gazettes)\n",
    "\n",
    "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
    "        for idx, answer in enumerate(tensor2list(labels)):\n",
    "            answers.extend([idx2tag[e].replace(\"_\", \"-\") for e in answer if idx2tag[e] != \"<SP>\" and idx2tag[e] != \"<PAD>\"])\n",
    "            predicts.extend([idx2tag[e].replace(\"_\", \"-\") for i, e in enumerate(output[idx]) if idx2tag[answer[i]] != \"<SP>\" and idx2tag[answer[i]] != \"<PAD>\"] )\n",
    "        \n",
    "    # 성능 평가\n",
    "    print(classification_report(answers, predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config):\n",
    "    # 모델 객체 생성\n",
    "    model = RNN_CRF(config).cuda()\n",
    "    # 단어 딕셔너리 생성\n",
    "    word2idx, idx2word = load_vocab(config[\"word_vocab_file\"])\n",
    "    tag2idx, idx2tag = load_vocab(config[\"tag_vocab_file\"])\n",
    "\n",
    "    # 저장된 가중치 Load\n",
    "    model.load_state_dict(torch.load(os.path.join(config[\"output_dir_path\"], config[\"trained_model_name\"])))\n",
    "\n",
    "    # 데이터 Load\n",
    "    ids, natural_inputs, test_input_features, test_gazettes = load_test_data(config, config[\"dev_file\"], word2idx, tag2idx)\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    # 불러온 데이터를 TensorDataset 객체로 변환\n",
    "    test_features = TensorDataset(test_input_features, test_gazettes)\n",
    "    test_dataloader = DataLoader(test_features, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        real_inputs = natural_inputs[step* batch_size: (step+1)*batch_size]\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        input_features, gazettes = batch\n",
    "        output = model(input_features, gazettes)\n",
    "\n",
    "        # 성능 평가를 위해 예측 값과 정답 값 리스트에 저장\n",
    "        for idx, label in enumerate(output):\n",
    "            label = ' '.join([idx2tag[e] for i, e in enumerate(label)][:len(real_inputs[idx].split(' '))])\n",
    "            with open('output.txt', 'a+', encoding='utf8') as outfile:\n",
    "                outfile.write(\"{}\\t{}\\t{}\\n\".format(ids[idx], real_inputs[idx], label))\n",
    "            \n",
    "def load_test_data(config, f_name, word2idx, tag2idx):\n",
    "    file = open(os.path.join(config['root_dir'], f_name),'r',encoding='utf8')\n",
    "    ids, original_lines = [], []\n",
    "    indexing_inputs = []\n",
    "    gazettes = []\n",
    "    print(\"{} file loading...\".format(f_name))\n",
    "\n",
    "    # 실제 데이터는 아래와 같은 형태를 가짐\n",
    "    # 문장 \\t 태그\n",
    "    # 세 종 대 왕 은 <SP> 조 선 의 <SP> 4 대 <SP> 왕 이 야 \\t B_PS I_PS I_PS I_PS O <SP> B_LC I_LC O <SP> O O <SP> O O O\n",
    "    \n",
    "    for line in tqdm(file.readlines()):\n",
    "        id, sentence, _ = line.strip().split('\\t')\n",
    "        ids.append(id)\n",
    "        original_lines.append(sentence)\n",
    "        \n",
    "        sentence = replace_digit(sentence)\n",
    "        input_sentence = convert_data2feature(sentence, word2idx, config[\"max_length\"])\n",
    "        input_gazette = gz.transform_end2end_complex_ngrams(sentence, config[\"max_length\"], config[\"ngrams\"])\n",
    "\n",
    "        indexing_inputs.append(input_sentence)\n",
    "        gazettes.append(input_gazette)\n",
    "        \n",
    "    indexing_inputs = torch.tensor(indexing_inputs, dtype=torch.long)\n",
    "    gazettes = torch.tensor(gazettes, dtype=torch.float)\n",
    "\n",
    "    return ids, original_lines, indexing_inputs, gazettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2158/2158 [00:00<00:00, 832457.28it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 57653.66it/s]\n",
      "  1%|          | 6/995 [00:00<00:17, 57.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab/word_vocab.txt vocab file loading...\n",
      "vocab/tag_vocab.txt vocab file loading...\n",
      "ner_dev.txt file loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [00:10<00:00, 94.42it/s]\n"
     ]
    }
   ],
   "source": [
    "config[\"trained_model_name\"] = \"epoch_{}.pt\".format(30)\n",
    "test(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413138,
     "status": "ok",
     "timestamp": 1604897797665,
     "user": {
      "displayName": "장영진",
      "photoUrl": "",
      "userId": "16685945690070219700"
     },
     "user_tz": -540
    },
    "id": "QXHsFV-z4zZc",
    "outputId": "8ea46d28-a59d-4e53-e1f7-088a9d24562c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2158/2158 [00:00<00:00, 1557080.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 56679.78it/s]\n",
      "  0%|          | 10/8314 [00:00<01:28, 93.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab/word_vocab.txt vocab file loading...\n",
      "vocab/tag_vocab.txt vocab file loading...\n",
      "ner_train_dev.txt file loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8314/8314 [01:29<00:00, 92.76it/s]\n",
      "  2%|▏         | 20/995 [00:00<00:10, 96.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_dev.txt file loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995/995 [00:10<00:00, 95.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 step processed.. current loss : 20.503459930419922\n",
      "100 step processed.. current loss : 15.52398681640625\n",
      "Average Loss : 26.8342545069181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/seqeval/metrics/sequence_labeling.py:45: UserWarning: <PAD> seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.70      0.67      0.68       622\n",
      "          LC       0.70      0.53      0.60       535\n",
      "          OG       0.60      0.53      0.56       971\n",
      "          PS       0.49      0.66      0.56       739\n",
      "          TI       0.64      0.53      0.58        95\n",
      "\n",
      "   micro avg       0.60      0.59      0.59      2962\n",
      "   macro avg       0.63      0.58      0.60      2962\n",
      "weighted avg       0.61      0.59      0.60      2962\n",
      "\n",
      "50 step processed.. current loss : 12.270347595214844\n",
      "100 step processed.. current loss : 5.8457489013671875\n",
      "Average Loss : 9.416201774890606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.81      0.74      0.78       622\n",
      "          LC       0.75      0.71      0.73       535\n",
      "          OG       0.75      0.64      0.69       971\n",
      "          PS       0.84      0.69      0.76       739\n",
      "          TI       0.85      0.76      0.80        95\n",
      "\n",
      "   micro avg       0.79      0.69      0.74      2962\n",
      "   macro avg       0.80      0.71      0.75      2962\n",
      "weighted avg       0.79      0.69      0.74      2962\n",
      "\n",
      "50 step processed.. current loss : 6.372114181518555\n",
      "100 step processed.. current loss : 8.534954071044922\n",
      "Average Loss : 6.182576913100022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.84      0.82      0.83       622\n",
      "          LC       0.82      0.80      0.81       535\n",
      "          OG       0.78      0.77      0.78       971\n",
      "          PS       0.89      0.79      0.84       739\n",
      "          TI       0.86      0.82      0.84        95\n",
      "\n",
      "   micro avg       0.83      0.79      0.81      2962\n",
      "   macro avg       0.84      0.80      0.82      2962\n",
      "weighted avg       0.83      0.79      0.81      2962\n",
      "\n",
      "50 step processed.. current loss : 4.548166275024414\n",
      "100 step processed.. current loss : 4.242803573608398\n",
      "Average Loss : 4.142022958168616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.89      0.81      0.85       622\n",
      "          LC       0.89      0.86      0.88       535\n",
      "          OG       0.87      0.78      0.82       971\n",
      "          PS       0.90      0.87      0.88       739\n",
      "          TI       0.89      0.80      0.84        95\n",
      "\n",
      "   micro avg       0.88      0.82      0.85      2962\n",
      "   macro avg       0.89      0.82      0.85      2962\n",
      "weighted avg       0.88      0.82      0.85      2962\n",
      "\n",
      "50 step processed.. current loss : 3.4299697875976562\n",
      "100 step processed.. current loss : 2.8354549407958984\n",
      "Average Loss : 2.808608000095074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.85      0.85      0.85       622\n",
      "          LC       0.91      0.90      0.91       535\n",
      "          OG       0.91      0.86      0.88       971\n",
      "          PS       0.94      0.91      0.92       739\n",
      "          TI       0.82      0.82      0.82        95\n",
      "\n",
      "   micro avg       0.90      0.88      0.89      2962\n",
      "   macro avg       0.88      0.87      0.88      2962\n",
      "weighted avg       0.90      0.88      0.89      2962\n",
      "\n",
      "50 step processed.. current loss : 1.1441059112548828\n",
      "100 step processed.. current loss : 1.9185295104980469\n",
      "Average Loss : 2.0294151416191686\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.90      0.86      0.88       622\n",
      "          LC       0.95      0.93      0.94       535\n",
      "          OG       0.94      0.90      0.92       971\n",
      "          PS       0.93      0.93      0.93       739\n",
      "          TI       0.92      0.86      0.89        95\n",
      "\n",
      "   micro avg       0.93      0.91      0.92      2962\n",
      "   macro avg       0.93      0.90      0.91      2962\n",
      "weighted avg       0.93      0.91      0.92      2962\n",
      "\n",
      "50 step processed.. current loss : 1.8569374084472656\n",
      "100 step processed.. current loss : 1.3442420959472656\n",
      "Average Loss : 1.472423312297234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.89      0.87      0.88       622\n",
      "          LC       0.97      0.93      0.95       535\n",
      "          OG       0.93      0.93      0.93       971\n",
      "          PS       0.96      0.95      0.95       739\n",
      "          TI       0.91      0.88      0.90        95\n",
      "\n",
      "   micro avg       0.94      0.92      0.93      2962\n",
      "   macro avg       0.93      0.91      0.92      2962\n",
      "weighted avg       0.94      0.92      0.93      2962\n",
      "\n",
      "50 step processed.. current loss : 0.8164424896240234\n",
      "100 step processed.. current loss : 0.7965812683105469\n",
      "Average Loss : 1.1034088584092947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.93      0.92      0.92       622\n",
      "          LC       0.96      0.95      0.96       535\n",
      "          OG       0.95      0.93      0.94       971\n",
      "          PS       0.97      0.95      0.96       739\n",
      "          TI       0.97      0.88      0.92        95\n",
      "\n",
      "   micro avg       0.95      0.93      0.94      2962\n",
      "   macro avg       0.95      0.93      0.94      2962\n",
      "weighted avg       0.95      0.93      0.94      2962\n",
      "\n",
      "50 step processed.. current loss : 1.3398771286010742\n",
      "100 step processed.. current loss : 0.8393154144287109\n",
      "Average Loss : 0.9719548129118406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.92      0.93       622\n",
      "          LC       0.96      0.96      0.96       535\n",
      "          OG       0.97      0.95      0.96       971\n",
      "          PS       0.96      0.96      0.96       739\n",
      "          TI       0.93      0.87      0.90        95\n",
      "\n",
      "   micro avg       0.96      0.94      0.95      2962\n",
      "   macro avg       0.95      0.93      0.94      2962\n",
      "weighted avg       0.96      0.94      0.95      2962\n",
      "\n",
      "50 step processed.. current loss : 1.0874338150024414\n",
      "100 step processed.. current loss : 0.8550891876220703\n",
      "Average Loss : 0.8172157205068148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.93      0.89      0.91       622\n",
      "          LC       0.98      0.96      0.97       535\n",
      "          OG       0.95      0.96      0.95       971\n",
      "          PS       0.98      0.96      0.97       739\n",
      "          TI       0.94      0.89      0.92        95\n",
      "\n",
      "   micro avg       0.96      0.94      0.95      2962\n",
      "   macro avg       0.96      0.93      0.94      2962\n",
      "weighted avg       0.96      0.94      0.95      2962\n",
      "\n",
      "50 step processed.. current loss : 0.6573600769042969\n",
      "100 step processed.. current loss : 0.769566535949707\n",
      "Average Loss : 0.7709580293068519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.93      0.94       622\n",
      "          LC       0.97      0.97      0.97       535\n",
      "          OG       0.96      0.95      0.96       971\n",
      "          PS       0.98      0.97      0.98       739\n",
      "          TI       0.97      0.92      0.94        95\n",
      "\n",
      "   micro avg       0.97      0.95      0.96      2962\n",
      "   macro avg       0.97      0.95      0.96      2962\n",
      "weighted avg       0.97      0.95      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.9452667236328125\n",
      "100 step processed.. current loss : 0.9650783538818359\n",
      "Average Loss : 0.7193518537741441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.94      0.95       622\n",
      "          LC       0.97      0.98      0.97       535\n",
      "          OG       0.96      0.97      0.96       971\n",
      "          PS       0.98      0.96      0.97       739\n",
      "          TI       0.82      0.91      0.86        95\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2962\n",
      "   macro avg       0.94      0.95      0.94      2962\n",
      "weighted avg       0.96      0.96      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.4812507629394531\n",
      "100 step processed.. current loss : 0.5610122680664062\n",
      "Average Loss : 0.6300268519383211\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.94      0.95       622\n",
      "          LC       0.98      0.99      0.98       535\n",
      "          OG       0.97      0.97      0.97       971\n",
      "          PS       0.99      0.97      0.98       739\n",
      "          TI       0.97      0.96      0.96        95\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2962\n",
      "   macro avg       0.97      0.97      0.97      2962\n",
      "weighted avg       0.97      0.97      0.97      2962\n",
      "\n",
      "50 step processed.. current loss : 0.6416740417480469\n",
      "100 step processed.. current loss : 0.4582176208496094\n",
      "Average Loss : 0.612582201224107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.94      0.92      0.93       622\n",
      "          LC       0.98      0.97      0.98       535\n",
      "          OG       0.97      0.96      0.97       971\n",
      "          PS       0.98      0.96      0.97       739\n",
      "          TI       1.00      0.96      0.98        95\n",
      "\n",
      "   micro avg       0.97      0.95      0.96      2962\n",
      "   macro avg       0.98      0.95      0.97      2962\n",
      "weighted avg       0.97      0.95      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.6410455703735352\n",
      "100 step processed.. current loss : 0.6395444869995117\n",
      "Average Loss : 0.6645618186547206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.96      0.94      0.95       622\n",
      "          LC       0.99      0.98      0.98       535\n",
      "          OG       0.98      0.96      0.97       971\n",
      "          PS       0.98      0.96      0.97       739\n",
      "          TI       0.96      0.93      0.94        95\n",
      "\n",
      "   micro avg       0.97      0.96      0.96      2962\n",
      "   macro avg       0.97      0.95      0.96      2962\n",
      "weighted avg       0.97      0.96      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7248353958129883\n",
      "100 step processed.. current loss : 0.593780517578125\n",
      "Average Loss : 0.5525413389389332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.98      0.95      0.96       622\n",
      "          LC       0.98      0.98      0.98       535\n",
      "          OG       0.98      0.97      0.98       971\n",
      "          PS       0.99      0.99      0.99       739\n",
      "          TI       0.95      0.96      0.95        95\n",
      "\n",
      "   micro avg       0.98      0.97      0.98      2962\n",
      "   macro avg       0.97      0.97      0.97      2962\n",
      "weighted avg       0.98      0.97      0.98      2962\n",
      "\n",
      "50 step processed.. current loss : 0.5006198883056641\n",
      "100 step processed.. current loss : 0.731043815612793\n",
      "Average Loss : 0.47539759622170374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.96      0.96       622\n",
      "          LC       0.99      0.96      0.97       535\n",
      "          OG       0.97      0.98      0.98       971\n",
      "          PS       0.99      0.97      0.98       739\n",
      "          TI       0.91      0.79      0.85        95\n",
      "\n",
      "   micro avg       0.98      0.96      0.97      2962\n",
      "   macro avg       0.97      0.93      0.95      2962\n",
      "weighted avg       0.98      0.96      0.97      2962\n",
      "\n",
      "50 step processed.. current loss : 0.3584880828857422\n",
      "100 step processed.. current loss : 0.3034651279449463\n",
      "Average Loss : 0.4210565053499662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.96      0.97      0.96       622\n",
      "          LC       0.99      0.99      0.99       535\n",
      "          OG       0.98      0.98      0.98       971\n",
      "          PS       0.98      0.99      0.99       739\n",
      "          TI       0.99      0.98      0.98        95\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2962\n",
      "   macro avg       0.98      0.98      0.98      2962\n",
      "weighted avg       0.98      0.98      0.98      2962\n",
      "\n",
      "50 step processed.. current loss : 0.35171985626220703\n",
      "100 step processed.. current loss : 0.43331050872802734\n",
      "Average Loss : 0.3048931536766199\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.98      0.97       622\n",
      "          LC       1.00      1.00      1.00       535\n",
      "          OG       0.99      0.99      0.99       971\n",
      "          PS       0.99      1.00      0.99       739\n",
      "          TI       0.99      1.00      0.99        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.99      0.99      0.99      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n",
      "50 step processed.. current loss : 0.08926951885223389\n",
      "100 step processed.. current loss : 0.20475494861602783\n",
      "Average Loss : 0.2536827176809311\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.98      0.98       622\n",
      "          LC       1.00      1.00      1.00       535\n",
      "          OG       0.99      0.99      0.99       971\n",
      "          PS       1.00      0.99      0.99       739\n",
      "          TI       0.98      0.94      0.96        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.99      0.98      0.98      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n",
      "50 step processed.. current loss : 0.240991473197937\n",
      "100 step processed.. current loss : 0.46218156814575195\n",
      "Average Loss : 0.2984787461849359\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.98      0.96      0.97       622\n",
      "          LC       0.99      0.99      0.99       535\n",
      "          OG       0.98      0.98      0.98       971\n",
      "          PS       0.99      0.99      0.99       739\n",
      "          TI       0.96      0.97      0.96        95\n",
      "\n",
      "   micro avg       0.99      0.98      0.98      2962\n",
      "   macro avg       0.98      0.98      0.98      2962\n",
      "weighted avg       0.99      0.98      0.98      2962\n",
      "\n",
      "50 step processed.. current loss : 0.40081003308296204\n",
      "100 step processed.. current loss : 0.6576521992683411\n",
      "Average Loss : 0.524777184942594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.94      0.95       622\n",
      "          LC       0.99      0.96      0.97       535\n",
      "          OG       0.95      0.97      0.96       971\n",
      "          PS       0.97      0.95      0.96       739\n",
      "          TI       0.90      0.91      0.90        95\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2962\n",
      "   macro avg       0.95      0.95      0.95      2962\n",
      "weighted avg       0.96      0.96      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.6836866140365601\n",
      "100 step processed.. current loss : 1.8895933628082275\n",
      "Average Loss : 1.0439648969815327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.93      0.95      0.94       622\n",
      "          LC       0.93      0.95      0.94       535\n",
      "          OG       0.93      0.94      0.94       971\n",
      "          PS       0.95      0.95      0.95       739\n",
      "          TI       0.90      0.89      0.90        95\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      2962\n",
      "   macro avg       0.93      0.94      0.93      2962\n",
      "weighted avg       0.94      0.94      0.94      2962\n",
      "\n",
      "50 step processed.. current loss : 1.599369764328003\n",
      "100 step processed.. current loss : 0.8734099268913269\n",
      "Average Loss : 1.0586670061716668\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.94      0.95       622\n",
      "          LC       0.98      0.97      0.97       535\n",
      "          OG       0.98      0.94      0.96       971\n",
      "          PS       0.96      0.96      0.96       739\n",
      "          TI       0.95      0.95      0.95        95\n",
      "\n",
      "   micro avg       0.97      0.95      0.96      2962\n",
      "   macro avg       0.96      0.95      0.96      2962\n",
      "weighted avg       0.97      0.95      0.96      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7943918704986572\n",
      "100 step processed.. current loss : 0.8461867570877075\n",
      "Average Loss : 0.7235112206293987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.95      0.95      0.95       622\n",
      "          LC       0.99      0.98      0.98       535\n",
      "          OG       0.96      0.97      0.96       971\n",
      "          PS       0.99      0.99      0.99       739\n",
      "          TI       0.91      0.95      0.93        95\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2962\n",
      "   macro avg       0.96      0.97      0.96      2962\n",
      "weighted avg       0.97      0.97      0.97      2962\n",
      "\n",
      "50 step processed.. current loss : 0.7100780606269836\n",
      "100 step processed.. current loss : 0.37068110704421997\n",
      "Average Loss : 0.4408660548237654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.98      0.97      0.97       622\n",
      "          LC       0.98      0.99      0.99       535\n",
      "          OG       0.98      0.98      0.98       971\n",
      "          PS       0.99      0.98      0.98       739\n",
      "          TI       0.97      0.96      0.96        95\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2962\n",
      "   macro avg       0.98      0.98      0.98      2962\n",
      "weighted avg       0.98      0.98      0.98      2962\n",
      "\n",
      "50 step processed.. current loss : 0.4236578941345215\n",
      "100 step processed.. current loss : 0.1311333179473877\n",
      "Average Loss : 0.3139148509846284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.98      0.97       622\n",
      "          LC       0.98      0.99      0.99       535\n",
      "          OG       0.99      0.99      0.99       971\n",
      "          PS       1.00      0.99      0.99       739\n",
      "          TI       0.98      0.97      0.97        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.98      0.98      0.98      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n",
      "50 step processed.. current loss : 0.8041447401046753\n",
      "100 step processed.. current loss : 0.3037419021129608\n",
      "Average Loss : 0.22352536825033334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.98      0.98      0.98       622\n",
      "          LC       1.00      0.99      1.00       535\n",
      "          OG       0.99      0.99      0.99       971\n",
      "          PS       0.99      0.99      0.99       739\n",
      "          TI       0.97      0.98      0.97        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.99      0.99      0.99      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n",
      "50 step processed.. current loss : 0.26371946930885315\n",
      "100 step processed.. current loss : 0.520124077796936\n",
      "Average Loss : 0.18134698392106938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.98      0.98       622\n",
      "          LC       0.99      1.00      1.00       535\n",
      "          OG       0.99      1.00      0.99       971\n",
      "          PS       1.00      0.99      1.00       739\n",
      "          TI       0.99      0.97      0.98        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.99      0.99      0.99      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n",
      "50 step processed.. current loss : 0.14971360564231873\n",
      "100 step processed.. current loss : 0.1524444818496704\n",
      "Average Loss : 0.18389420681274854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DT       0.97      0.98      0.98       622\n",
      "          LC       1.00      1.00      1.00       535\n",
      "          OG       0.99      0.99      0.99       971\n",
      "          PS       0.99      0.99      0.99       739\n",
      "          TI       0.98      0.95      0.96        95\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2962\n",
      "   macro avg       0.99      0.98      0.98      2962\n",
      "weighted avg       0.99      0.99      0.99      2962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                                                        #\n",
    "#        평가 기준이 되는 지표는 Macro F1 Score          #\n",
    "#           제출 포맷은 id \\t predict_tag                #\n",
    "#            25 \\t B_PS I_PS <SP> O O O ...              #\n",
    "#                                                        #\n",
    "##########################################################\n",
    "\n",
    "\n",
    "import os\n",
    "if(__name__==\"__main__\"):\n",
    "    output_dir = os.path.join(config['root_dir'], \"output\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if(config[\"mode\"] == \"train\"):\n",
    "        train(config)\n",
    "    else:\n",
    "        test(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPPhYT+qI42fZ4mJM5g2+tr",
   "collapsed_sections": [],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
